{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mrjob in /Users/Karez/opt/anaconda3/lib/python3.9/site-packages (0.7.4)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /Users/Karez/opt/anaconda3/lib/python3.9/site-packages (from mrjob) (6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting count_line.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile count_line.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRlineCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield None, 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield \"sum of lines:\", sum(values)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRlineCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting count_line_and_chars.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile count_line_and_chars.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRLineAndCharCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        yield \"lines\", 1\n",
    "        yield \"chars\", len(line.strip())\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRLineAndCharCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting count_line_and_chars_words.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile count_line_and_chars_words.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRLineAndCharCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        yield \"lines\", 1\n",
    "        yield \"chars\", len(line.strip())\n",
    "        yield \"words\", len(line.split())\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRLineAndCharCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MRJob in module mrjob.job:\n",
      "\n",
      "class MRJob(builtins.object)\n",
      " |  MRJob(args=None)\n",
      " |  \n",
      " |  The base class for all MapReduce jobs. See :py:meth:`__init__`\n",
      " |  for details.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, args=None)\n",
      " |      Entry point for running your job from other Python code.\n",
      " |      \n",
      " |      You can pass in command-line arguments, and the job will act the same\n",
      " |      way it would if it were run from the command line. For example, to\n",
      " |      run your job on EMR::\n",
      " |      \n",
      " |          mr_job = MRYourJob(args=['-r', 'emr'])\n",
      " |          with mr_job.make_runner() as runner:\n",
      " |              ...\n",
      " |      \n",
      " |      Passing in ``None`` is the same as passing in ``sys.argv[1:]``\n",
      " |      \n",
      " |      For a full list of command-line arguments, run:\n",
      " |      ``python -m mrjob.job --help``\n",
      " |      \n",
      " |      :param args: Arguments to your script (switches and input files)\n",
      " |      \n",
      " |      .. versionchanged:: 0.7.0\n",
      " |      \n",
      " |         Previously, *args* set to ``None`` was equivalent to ``[]``.\n",
      " |  \n",
      " |  add_file_arg(self, *args, **kwargs)\n",
      " |      Add a command-line option that sends an external file\n",
      " |      (e.g. a SQLite DB) to Hadoop::\n",
      " |      \n",
      " |           def configure_args(self):\n",
      " |              super(MRYourJob, self).configure_args()\n",
      " |              self.add_file_arg('--scoring-db', help=...)\n",
      " |      \n",
      " |      This does the right thing: the file will be uploaded to the working\n",
      " |      dir of the script on Hadoop, and the script will be passed the same\n",
      " |      option, but with the local name of the file in the script's working\n",
      " |      directory.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |         If you pass a file to a job, best practice is to lazy-load its\n",
      " |         contents (e.g. make a method that opens the file the first time\n",
      " |         you call it) rather than loading it in your job's constructor or\n",
      " |         :py:meth:`load_args`. Not only is this more efficient, it's\n",
      " |         necessary if you want to run your job in a Spark executor\n",
      " |         (because the file may not be in the same place in a Spark driver).\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |         We suggest against sending Berkeley DBs to your job, as\n",
      " |         Berkeley DB is not forwards-compatible (so a Berkeley DB that you\n",
      " |         construct on your computer may not be readable from within\n",
      " |         Hadoop). Use SQLite databases instead. If all you need is an on-disk\n",
      " |         hash table, try out the :py:mod:`sqlite3dbm` module.\n",
      " |      \n",
      " |      .. versionchanged:: 0.6.6\n",
      " |      \n",
      " |         now accepts explicit ``type=str``\n",
      " |      \n",
      " |      .. versionchanged:: 0.6.8\n",
      " |      \n",
      " |         fully supported on Spark, including ``local[*]`` master\n",
      " |  \n",
      " |  add_passthru_arg(self, *args, **kwargs)\n",
      " |      Function to create options which both the job runner\n",
      " |      and the job itself respect (we use this for protocols, for example).\n",
      " |      \n",
      " |      Use it like you would use\n",
      " |      :py:func:`argparse.ArgumentParser.add_argument`::\n",
      " |      \n",
      " |          def configure_args(self):\n",
      " |              super(MRYourJob, self).configure_args()\n",
      " |              self.add_passthru_arg(\n",
      " |                  '--max-ngram-size', type=int, default=4, help='...')\n",
      " |      \n",
      " |      If you want to pass files through to the mapper/reducer, use\n",
      " |      :py:meth:`add_file_arg` instead.\n",
      " |      \n",
      " |      If you want to pass through a built-in option (e.g. ``--runner``, use\n",
      " |      :py:meth:`pass_arg_through` instead.\n",
      " |  \n",
      " |  archives(self)\n",
      " |      Like :py:attr:`ARCHIVES`, except that it can return a dynamically\n",
      " |      generated list of archives to upload and unpack. Overriding\n",
      " |      this method disables :py:attr:`ARCHIVES`.\n",
      " |      \n",
      " |      Paths returned by this method are relative to the working directory\n",
      " |      (not the script). Note that the job runner will *always* expand\n",
      " |      environment variables and ``~`` in paths returned by this method.\n",
      " |      \n",
      " |      You do not have to worry about inadvertently disabling ``--archives``;\n",
      " |      this switch is handled separately.\n",
      " |      \n",
      " |      .. versionadded:: 0.6.4\n",
      " |  \n",
      " |  combine_pairs(self, pairs, step_num=0)\n",
      " |      Runs :py:meth:`combiner_init`,\n",
      " |      :py:meth:`combiner`, and :py:meth:`combiner_final`\n",
      " |      for one reduce task in one step.\n",
      " |      \n",
      " |      Takes in a sequence of (key, value) pairs as input, and yields\n",
      " |      (key, value) pairs as output.\n",
      " |      \n",
      " |      :py:meth:`run_combiner` essentially wraps this method with code to\n",
      " |      handle reading/decoding input and writing/encoding output.\n",
      " |      \n",
      " |      .. versionadded:: 0.6.7\n",
      " |  \n",
      " |  combiner(self, key, values)\n",
      " |      Re-define this to define the combiner for a one-step job.\n",
      " |      \n",
      " |      Yields one or more tuples of ``(out_key, out_value)``\n",
      " |      \n",
      " |      :param key: A key which was yielded by the mapper\n",
      " |      :param value: A generator which yields all values yielded by one mapper\n",
      " |                    task/node which correspond to ``key``.\n",
      " |      \n",
      " |      By default (if you don't mess with :ref:`job-protocols`):\n",
      " |       - ``out_key`` and ``out_value`` must be JSON-encodable.\n",
      " |       - ``key`` and ``value`` will have been decoded from JSON (so tuples\n",
      " |         will become lists).\n",
      " |  \n",
      " |  combiner_cmd(self)\n",
      " |      Re-define this to define the combiner for a one-step job **as a\n",
      " |      shell command.** If you define your mapper this way, the command will\n",
      " |      be passed unchanged to Hadoop Streaming, with some minor exceptions.\n",
      " |      For specifics, see :ref:`cmd-steps`.\n",
      " |      \n",
      " |      Basic example::\n",
      " |      \n",
      " |          def combiner_cmd(self):\n",
      " |              return 'cat'\n",
      " |  \n",
      " |  combiner_final(self)\n",
      " |      Re-define this to define an action to run after the combiner reaches\n",
      " |      the end of input.\n",
      " |      \n",
      " |      Yields one or more tuples of ``(out_key, out_value)``.\n",
      " |      \n",
      " |      By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n",
      " |      re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n",
      " |  \n",
      " |  combiner_init(self)\n",
      " |      Re-define this to define an action to run before the combiner\n",
      " |      processes any input.\n",
      " |      \n",
      " |      One use for this function is to initialize combiner-specific helper\n",
      " |      structures.\n",
      " |      \n",
      " |      Yields one or more tuples of ``(out_key, out_value)``.\n",
      " |      \n",
      " |      By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n",
      " |      re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n",
      " |  \n",
      " |  combiner_pre_filter(self)\n",
      " |      Re-define this to specify a shell command to filter the combiner's\n",
      " |      input before it gets to your job's combiner in a one-step job. For\n",
      " |      important specifics, see :ref:`cmd-filters`.\n",
      " |      \n",
      " |      Basic example::\n",
      " |      \n",
      " |          def combiner_pre_filter(self):\n",
      " |              return 'grep \"ponies\"'\n",
      " |  \n",
      " |  configure_args(self)\n",
      " |      Define arguments for this script. Called from :py:meth:`__init__()`.\n",
      " |      \n",
      " |      Re-define to define custom command-line arguments or pass\n",
      " |      through existing ones::\n",
      " |      \n",
      " |          def configure_args(self):\n",
      " |              super(MRYourJob, self).configure_args()\n",
      " |      \n",
      " |              self.add_passthru_arg(...)\n",
      " |              self.add_file_arg(...)\n",
      " |              self.pass_arg_through(...)\n",
      " |              ...\n",
      " |  \n",
      " |  dirs(self)\n",
      " |      Like :py:attr:`DIRS`, except that it can return a dynamically\n",
      " |      generated list of directories to upload. Overriding\n",
      " |      this method disables :py:attr:`DIRS`.\n",
      " |      \n",
      " |      Paths returned by this method are relative to the working directory\n",
      " |      (not the script). Note that the job runner will *always* expand\n",
      " |      environment variables and ``~`` in paths returned by this method.\n",
      " |      \n",
      " |      You do not have to worry about inadvertently disabling ``--dirs``;\n",
      " |      this switch is handled separately.\n",
      " |      \n",
      " |      .. versionadded:: 0.6.4\n",
      " |  \n",
      " |  execute(self)\n",
      " |  \n",
      " |  files(self)\n",
      " |      Like :py:attr:`FILES`, except that it can return a dynamically\n",
      " |      generated list of files to upload. Overriding\n",
      " |      this method disables :py:attr:`FILES`.\n",
      " |      \n",
      " |      Paths returned by this method are relative to the working directory\n",
      " |      (not the script). Note that the job runner will *always* expand\n",
      " |      environment variables and ``~`` in paths returned by this method.\n",
      " |      \n",
      " |      You do not have to worry about inadvertently disabling ``--files``;\n",
      " |      this switch is handled separately.\n",
      " |      \n",
      " |      .. versionadded:: 0.6.4\n",
      " |  \n",
      " |  hadoop_input_format(self)\n",
      " |      Optional Hadoop ``InputFormat`` class to parse input for\n",
      " |      the first step of the job.\n",
      " |      \n",
      " |      Normally, setting :py:attr:`HADOOP_INPUT_FORMAT` is sufficient;\n",
      " |      redefining this method is only for when you want to get fancy.\n",
      " |  \n",
      " |  hadoop_output_format(self)\n",
      " |      Optional Hadoop ``OutputFormat`` class to write output for\n",
      " |      the last step of the job.\n",
      " |      \n",
      " |      Normally, setting :py:attr:`HADOOP_OUTPUT_FORMAT` is sufficient;\n",
      " |      redefining this method is only for when you want to get fancy.\n",
      " |  \n",
      " |  increment_counter(self, group, counter, amount=1)\n",
      " |      Increment a counter in Hadoop streaming by printing to stderr.\n",
      " |      \n",
      " |      :type group: str\n",
      " |      :param group: counter group\n",
      " |      :type counter: str\n",
      " |      :param counter: description of the counter\n",
      " |      :type amount: int\n",
      " |      :param amount: how much to increment the counter by\n",
      " |      \n",
      " |      Commas in ``counter`` or ``group`` will be automatically replaced\n",
      " |      with semicolons (commas confuse Hadoop streaming).\n",
      " |  \n",
      " |  input_protocol(self)\n",
      " |      Instance of the protocol to use to convert input lines to Python\n",
      " |      objects. Default behavior is to return an instance of\n",
      " |      :py:attr:`INPUT_PROTOCOL`.\n",
      " |  \n",
      " |  internal_protocol(self)\n",
      " |      Instance of the protocol to use to communicate between steps.\n",
      " |      Default behavior is to return an instance of\n",
      " |      :py:attr:`INTERNAL_PROTOCOL`.\n",
      " |  \n",
      " |  is_task(self)\n",
      " |      True if this is a mapper, combiner, reducer, or Spark script.\n",
      " |      \n",
      " |      This is mostly useful inside :py:meth:`load_args`, to disable\n",
      " |      loading args when we aren't running inside Hadoop.\n",
      " |  \n",
      " |  jobconf(self)\n",
      " |      ``-D`` args to pass to hadoop streaming. This should be a map\n",
      " |      from property name to value. By default, returns :py:attr:`JOBCONF`.\n",
      " |      \n",
      " |      .. versionchanged:: 0.6.6\n",
      " |      \n",
      " |         re-defining longer clobbers command-line\n",
      " |         ``--jobconf`` options.\n",
      " |  \n",
      " |  libjars(self)\n",
      " |      Optional list of paths of jar files to run our job with using\n",
      " |      Hadoop's ``-libjars`` option. Normally setting :py:attr:`LIBJARS`\n",
      " |      is sufficient. Paths from :py:attr:`LIBJARS` are interpreted as\n",
      " |      relative to the the directory containing the script (paths from the\n",
      " |      command-line are relative to the current working directory).\n",
      " |      \n",
      " |      Note that ``~`` and environment variables in paths will always be\n",
      " |      expanded by the job runner (see :mrjob-opt:`libjars`).\n",
      " |      \n",
      " |      .. versionchanged:: 0.6.6\n",
      " |      \n",
      " |         re-defining this no longer clobbers the command-line\n",
      " |         ``--libjars`` option\n",
      " |  \n",
      " |  load_args(self, args)\n",
      " |      Load command-line options into ``self.options``.\n",
      " |      \n",
      " |      Called from :py:meth:`__init__()` after :py:meth:`configure_args`.\n",
      " |      \n",
      " |      :type args: list of str\n",
      " |      :param args: a list of command line arguments. ``None`` will be\n",
      " |                   treated the same as ``[]``.\n",
      " |      \n",
      " |      Re-define if you want to post-process command-line arguments::\n",
      " |      \n",
      " |          def load_args(self, args):\n",
      " |              super(MRYourJob, self).load_args(args)\n",
      " |      \n",
      " |              self.stop_words = self.options.stop_words.split(',')\n",
      " |              ...\n",
      " |  \n",
      " |  make_runner(self)\n",
      " |      Make a runner based on command-line arguments, so we can\n",
      " |      launch this job on EMR, on Hadoop, or locally.\n",
      " |      \n",
      " |      :rtype: :py:class:`mrjob.runner.MRJobRunner`\n",
      " |  \n",
      " |  map_pairs(self, pairs, step_num=0)\n",
      " |      Runs :py:meth:`mapper_init`,\n",
      " |      :py:meth:`mapper`/:py:meth:`mapper_raw`, and :py:meth:`mapper_final`\n",
      " |      for one map task in one step.\n",
      " |      \n",
      " |      Takes in a sequence of (key, value) pairs as input, and yields\n",
      " |      (key, value) pairs as output.\n",
      " |      \n",
      " |      :py:meth:`run_mapper` essentially wraps this method with code to handle\n",
      " |      reading/decoding input and writing/encoding output.\n",
      " |      \n",
      " |      .. versionadded:: 0.6.7\n",
      " |  \n",
      " |  mapper(self, key, value)\n",
      " |      Re-define this to define the mapper for a one-step job.\n",
      " |      \n",
      " |      Yields zero or more tuples of ``(out_key, out_value)``.\n",
      " |      \n",
      " |      :param key: A value parsed from input.\n",
      " |      :param value: A value parsed from input.\n",
      " |      \n",
      " |      If you don't re-define this, your job will have a mapper that simply\n",
      " |      yields ``(key, value)`` as-is.\n",
      " |      \n",
      " |      By default (if you don't mess with :ref:`job-protocols`):\n",
      " |       - ``key`` will be ``None``\n",
      " |       - ``value`` will be the raw input line, with newline stripped.\n",
      " |       - ``out_key`` and ``out_value`` must be JSON-encodable: numeric,\n",
      " |         unicode, boolean, ``None``, list, or dict whose keys are unicodes.\n",
      " |  \n",
      " |  mapper_cmd(self)\n",
      " |      Re-define this to define the mapper for a one-step job **as a shell\n",
      " |      command.** If you define your mapper this way, the command will be\n",
      " |      passed unchanged to Hadoop Streaming, with some minor exceptions. For\n",
      " |      important specifics, see :ref:`cmd-steps`.\n",
      " |      \n",
      " |      Basic example::\n",
      " |      \n",
      " |          def mapper_cmd(self):\n",
      " |              return 'cat'\n",
      " |  \n",
      " |  mapper_final(self)\n",
      " |      Re-define this to define an action to run after the mapper reaches\n",
      " |      the end of input.\n",
      " |      \n",
      " |      One way to use this is to store a total in an instance variable, and\n",
      " |      output it after reading all input data. See :py:mod:`mrjob.examples`\n",
      " |      for an example.\n",
      " |      \n",
      " |      Yields one or more tuples of ``(out_key, out_value)``.\n",
      " |      \n",
      " |      By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n",
      " |      re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n",
      " |  \n",
      " |  mapper_init(self)\n",
      " |      Re-define this to define an action to run before the mapper\n",
      " |      processes any input.\n",
      " |      \n",
      " |      One use for this function is to initialize mapper-specific helper\n",
      " |      structures.\n",
      " |      \n",
      " |      Yields one or more tuples of ``(out_key, out_value)``.\n",
      " |      \n",
      " |      By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n",
      " |      re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n",
      " |  \n",
      " |  mapper_pre_filter(self)\n",
      " |      Re-define this to specify a shell command to filter the mapper's\n",
      " |      input before it gets to your job's mapper in a one-step job. For\n",
      " |      important specifics, see :ref:`cmd-filters`.\n",
      " |      \n",
      " |      Basic example::\n",
      " |      \n",
      " |          def mapper_pre_filter(self):\n",
      " |              return 'grep \"ponies\"'\n",
      " |  \n",
      " |  mapper_raw(self, input_path, input_uri)\n",
      " |      Re-define this to make Hadoop pass one input file to each\n",
      " |      mapper.\n",
      " |      \n",
      " |      :param input_path: a local path that the input file has been copied to\n",
      " |      :param input_uri: the URI of the input file on HDFS, S3, etc\n",
      " |      \n",
      " |      .. versionadded:: 0.6.3\n",
      " |  \n",
      " |  output_protocol(self)\n",
      " |      Instance of the protocol to use to convert Python objects to output\n",
      " |      lines. Default behavior is to return an instance of\n",
      " |      :py:attr:`OUTPUT_PROTOCOL`.\n",
      " |  \n",
      " |  parse_output(self, chunks)\n",
      " |      Parse the final output of this MRJob (as a stream of byte chunks)\n",
      " |      into a stream of ``(key, value)``.\n",
      " |  \n",
      " |  partitioner(self)\n",
      " |      Optional Hadoop partitioner class to use to determine how mapper\n",
      " |      output should be sorted and distributed to reducers.\n",
      " |      \n",
      " |      By default, returns :py:attr:`PARTITIONER`.\n",
      " |      \n",
      " |      You probably don't need to re-define this; it's just here for\n",
      " |      completeness.\n",
      " |  \n",
      " |  pass_arg_through(self, opt_str)\n",
      " |      Pass the given argument through to the job.\n",
      " |  \n",
      " |  pick_protocols(self, step_num, step_type)\n",
      " |      Pick the protocol classes to use for reading and writing for the\n",
      " |      given step.\n",
      " |      \n",
      " |      :type step_num: int\n",
      " |      :param step_num: which step to run (e.g. ``0`` for the first step)\n",
      " |      :type step_type: str\n",
      " |      :param step_type: one of `'mapper'`, `'combiner'`, or `'reducer'`\n",
      " |      :return: (read_function, write_function)\n",
      " |      \n",
      " |      By default, we use one protocol for reading input, one\n",
      " |      internal protocol for communication between steps, and one\n",
      " |      protocol for final output (which is usually the same as the\n",
      " |      internal protocol). Protocols can be controlled by setting\n",
      " |      :py:attr:`INPUT_PROTOCOL`, :py:attr:`INTERNAL_PROTOCOL`, and\n",
      " |      :py:attr:`OUTPUT_PROTOCOL`.\n",
      " |      \n",
      " |      Re-define this if you need fine control over which protocols\n",
      " |      are used by which steps.\n",
      " |  \n",
      " |  reduce_pairs(self, pairs, step_num=0)\n",
      " |      Runs :py:meth:`reducer_init`,\n",
      " |      :py:meth:`reducer`, and :py:meth:`reducer_final`\n",
      " |      for one reduce task in one step.\n",
      " |      \n",
      " |      Takes in a sequence of (key, value) pairs as input, and yields\n",
      " |      (key, value) pairs as output.\n",
      " |      \n",
      " |      :py:meth:`run_reducer` essentially wraps this method with code to\n",
      " |      handle reading/decoding input and writing/encoding output.\n",
      " |      \n",
      " |      .. versionadded:: 0.6.7\n",
      " |  \n",
      " |  reducer(self, key, values)\n",
      " |      Re-define this to define the reducer for a one-step job.\n",
      " |      \n",
      " |      Yields one or more tuples of ``(out_key, out_value)``\n",
      " |      \n",
      " |      :param key: A key which was yielded by the mapper\n",
      " |      :param value: A generator which yields all values yielded by the\n",
      " |                    mapper which correspond to ``key``.\n",
      " |      \n",
      " |      By default (if you don't mess with :ref:`job-protocols`):\n",
      " |       - ``out_key`` and ``out_value`` must be JSON-encodable.\n",
      " |       - ``key`` and ``value`` will have been decoded from JSON (so tuples\n",
      " |         will become lists).\n",
      " |  \n",
      " |  reducer_cmd(self)\n",
      " |      Re-define this to define the reducer for a one-step job **as a shell\n",
      " |      command.** If you define your mapper this way, the command will be\n",
      " |      passed unchanged to Hadoop Streaming, with some minor exceptions. For\n",
      " |      specifics, see :ref:`cmd-steps`.\n",
      " |      \n",
      " |      Basic example::\n",
      " |      \n",
      " |          def reducer_cmd(self):\n",
      " |              return 'cat'\n",
      " |  \n",
      " |  reducer_final(self)\n",
      " |      Re-define this to define an action to run after the reducer reaches\n",
      " |      the end of input.\n",
      " |      \n",
      " |      Yields one or more tuples of ``(out_key, out_value)``.\n",
      " |      \n",
      " |      By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n",
      " |      re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n",
      " |  \n",
      " |  reducer_init(self)\n",
      " |      Re-define this to define an action to run before the reducer\n",
      " |      processes any input.\n",
      " |      \n",
      " |      One use for this function is to initialize reducer-specific helper\n",
      " |      structures.\n",
      " |      \n",
      " |      Yields one or more tuples of ``(out_key, out_value)``.\n",
      " |      \n",
      " |      By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n",
      " |      re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n",
      " |  \n",
      " |  reducer_pre_filter(self)\n",
      " |      Re-define this to specify a shell command to filter the reducer's\n",
      " |      input before it gets to your job's reducer in a one-step job. For\n",
      " |      important specifics, see :ref:`cmd-filters`.\n",
      " |      \n",
      " |      Basic example::\n",
      " |      \n",
      " |          def reducer_pre_filter(self):\n",
      " |              return 'grep \"ponies\"'\n",
      " |  \n",
      " |  run_combiner(self, step_num=0)\n",
      " |      Run the combiner for the given step.\n",
      " |      \n",
      " |      :type step_num: int\n",
      " |      :param step_num: which step to run (0-indexed)\n",
      " |      \n",
      " |      If we encounter a line that can't be decoded by our input protocol,\n",
      " |      or a tuple that can't be encoded by our output protocol, we'll\n",
      " |      increment a counter rather than raising an exception. If\n",
      " |      --strict-protocols is set, then an exception is raised\n",
      " |      \n",
      " |      Called from :py:meth:`run`. You'd probably only want to call this\n",
      " |      directly from automated tests.\n",
      " |  \n",
      " |  run_job(self)\n",
      " |      Run the all steps of the job, logging errors (and debugging output\n",
      " |      if :option:`--verbose` is specified) to STDERR and streaming the\n",
      " |      output to STDOUT.\n",
      " |      \n",
      " |      Called from :py:meth:`run`. You'd probably only want to call this\n",
      " |      directly from automated tests.\n",
      " |  \n",
      " |  run_mapper(self, step_num=0)\n",
      " |      Run the mapper and final mapper action for the given step.\n",
      " |      \n",
      " |      :type step_num: int\n",
      " |      :param step_num: which step to run (0-indexed)\n",
      " |      \n",
      " |      Called from :py:meth:`run`. You'd probably only want to call this\n",
      " |      directly from automated tests.\n",
      " |  \n",
      " |  run_reducer(self, step_num=0)\n",
      " |      Run the reducer for the given step.\n",
      " |      \n",
      " |      :type step_num: int\n",
      " |      :param step_num: which step to run (0-indexed)\n",
      " |      \n",
      " |      Called from :py:meth:`run`. You'd probably only want to call this\n",
      " |      directly from automated tests.\n",
      " |  \n",
      " |  run_spark(self, step_num)\n",
      " |      Run the Spark code for the given step.\n",
      " |      \n",
      " |      :type step_num: int\n",
      " |      :param step_num: which step to run (0-indexed)\n",
      " |      \n",
      " |      Called from :py:meth:`run`. You'd probably only want to call this\n",
      " |      directly from automated tests.\n",
      " |  \n",
      " |  sandbox(self, stdin=None, stdout=None, stderr=None)\n",
      " |      Redirect stdin, stdout, and stderr for automated testing.\n",
      " |      \n",
      " |      You can set stdin, stdout, and stderr to file objects. By\n",
      " |      default, they'll be set to empty ``BytesIO`` objects.\n",
      " |      You can then access the job's file handles through ``self.stdin``,\n",
      " |      ``self.stdout``, and ``self.stderr``. See :ref:`testing` for more\n",
      " |      information about testing.\n",
      " |      \n",
      " |      You may call sandbox multiple times (this will essentially clear\n",
      " |      the file handles).\n",
      " |      \n",
      " |      ``stdin`` is empty by default. You can set it to anything that yields\n",
      " |      lines::\n",
      " |      \n",
      " |          mr_job.sandbox(stdin=BytesIO(b'some_data\\n'))\n",
      " |      \n",
      " |      or, equivalently::\n",
      " |      \n",
      " |          mr_job.sandbox(stdin=[b'some_data\\n'])\n",
      " |      \n",
      " |      For convenience, this sandbox() returns self, so you can do::\n",
      " |      \n",
      " |          mr_job = MRJobClassToTest().sandbox()\n",
      " |      \n",
      " |      Simple testing example::\n",
      " |      \n",
      " |          mr_job = MRYourJob.sandbox()\n",
      " |          self.assertEqual(list(mr_job.reducer('foo', ['a', 'b'])), [...])\n",
      " |      \n",
      " |      More complex testing example::\n",
      " |      \n",
      " |          from BytesIO import BytesIO\n",
      " |      \n",
      " |          from mrjob.parse import parse_mr_job_stderr\n",
      " |          from mrjob.protocol import JSONProtocol\n",
      " |      \n",
      " |          mr_job = MRYourJob(args=[...])\n",
      " |      \n",
      " |          fake_input = '\"foo\"\\t\"bar\"\\n\"foo\"\\t\"baz\"\\n'\n",
      " |          mr_job.sandbox(stdin=BytesIO(fake_input))\n",
      " |      \n",
      " |          mr_job.run_reducer(link_num=0)\n",
      " |      \n",
      " |          self.assertEqual(mrjob.stdout.getvalue(), ...)\n",
      " |          self.assertEqual(parse_mr_job_stderr(mr_job.stderr), ...)\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |         If you are using Spark, it's recommended you only pass in\n",
      " |         :py:class:`io.BytesIO` or other serializable alternatives to file\n",
      " |         objects. *stdin*, *stdout*, and *stderr* get stored as job\n",
      " |         attributes, which means if they aren't serializable, neither\n",
      " |         is the job instance or its methods.\n",
      " |  \n",
      " |  set_status(self, msg)\n",
      " |      Set the job status in hadoop streaming by printing to stderr.\n",
      " |      \n",
      " |      This is also a good way of doing a keepalive for a job that goes a\n",
      " |      long time between outputs; Hadoop streaming usually times out jobs\n",
      " |      that give no output for longer than 10 minutes.\n",
      " |  \n",
      " |  sort_values(self)\n",
      " |      A method that by default, just returns the value of\n",
      " |      :py:attr:`SORT_VALUES`. Mostly exists for the sake\n",
      " |      of consistency, but you could override it if you wanted to make\n",
      " |      secondary sort configurable.\n",
      " |  \n",
      " |  spark(self, input_path, output_path)\n",
      " |      Re-define this with Spark code to run. You can read input\n",
      " |      with *input_path* and output with *output_path*.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |         Prior to v0.6.8, to pass job methods into Spark\n",
      " |         (``rdd.flatMap(self.some_method)``), you first had to call\n",
      " |         :py:meth:`self.sandbox() <mrjob.job.MRJob.sandbox>`; otherwise\n",
      " |         Spark would error because *self* was not serializable.\n",
      " |  \n",
      " |  spark_args(self)\n",
      " |      Redefine this to pass custom arguments to Spark.\n",
      " |  \n",
      " |  steps(self)\n",
      " |      Re-define this to make a multi-step job.\n",
      " |      \n",
      " |      If you don't re-define this, we'll automatically create a one-step\n",
      " |      job using any of :py:meth:`mapper`, :py:meth:`mapper_init`,\n",
      " |      :py:meth:`mapper_final`, :py:meth:`reducer_init`,\n",
      " |      :py:meth:`reducer_final`, and :py:meth:`reducer` that you've\n",
      " |      re-defined. For example::\n",
      " |      \n",
      " |          def steps(self):\n",
      " |              return [MRStep(mapper=self.transform_input,\n",
      " |                             reducer=self.consolidate_1),\n",
      " |                      MRStep(reducer_init=self.log_mapper_init,\n",
      " |                             reducer=self.consolidate_2)]\n",
      " |      \n",
      " |      :return: a list of steps constructed with\n",
      " |               :py:class:`~mrjob.step.MRStep` or other classes in\n",
      " |               :py:mod:`mrjob.step`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  mr_job_script() from builtins.type\n",
      " |      Path of this script. This returns the file containing\n",
      " |      this class, or ``None`` if there isn't any (e.g. it was\n",
      " |      defined from the command line interface.)\n",
      " |  \n",
      " |  run() from builtins.type\n",
      " |      Entry point for running job from the command-line.\n",
      " |      \n",
      " |      This is also the entry point when a mapper or reducer is run\n",
      " |      by Hadoop Streaming.\n",
      " |      \n",
      " |      Does one of:\n",
      " |      \n",
      " |      * Run a mapper (:option:`--mapper`). See :py:meth:`run_mapper`\n",
      " |      * Run a combiner (:option:`--combiner`). See :py:meth:`run_combiner`\n",
      " |      * Run a reducer (:option:`--reducer`). See :py:meth:`run_reducer`\n",
      " |      * Run the entire job. See :py:meth:`run_job`\n",
      " |  \n",
      " |  set_up_logging(quiet=False, verbose=False, stream=None) from builtins.type\n",
      " |      Set up logging when running from the command line. This is also\n",
      " |      used by the various command-line utilities.\n",
      " |      \n",
      " |      :param bool quiet: If true, don't log. Overrides *verbose*.\n",
      " |      :param bool verbose: If true, set log level to ``DEBUG`` (default is\n",
      " |                           ``INFO``)\n",
      " |      :param bool stream: Stream to log to (default is ``sys.stderr``)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  stderr\n",
      " |  \n",
      " |  stdin\n",
      " |  \n",
      " |  stdout\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ARCHIVES = []\n",
      " |  \n",
      " |  DIRS = []\n",
      " |  \n",
      " |  FILES = []\n",
      " |  \n",
      " |  HADOOP_INPUT_FORMAT = None\n",
      " |  \n",
      " |  HADOOP_OUTPUT_FORMAT = None\n",
      " |  \n",
      " |  INPUT_PROTOCOL = <class 'mrjob.protocol.TextValueProtocol'>\n",
      " |      Attempt to UTF-8 decode line (without trailing newline) into ``value``,\n",
      " |      falling back to latin-1. (``key`` is always ``None``). Output ``value``\n",
      " |      UTF-8 encoded, discarding ``key``.\n",
      " |      \n",
      " |      **This is the default protocol used by jobs to read input on Python 3.**\n",
      " |      \n",
      " |      This is a good solution for reading text files which are mostly ASCII but\n",
      " |      may have some other bytes of unknown encoding (e.g. logs).\n",
      " |      \n",
      " |      If you wish to enforce a particular encoding, use\n",
      " |      :py:class:`BytesValueProtocol` instead::\n",
      " |      \n",
      " |          class MREncodingEnforcer(MRJob):\n",
      " |      \n",
      " |              INPUT_PROTOCOL = BytesValueProtocol\n",
      " |      \n",
      " |              def mapper(self, _, value):\n",
      " |                  value = value.decode('utf_8')\n",
      " |                  ...\n",
      " |  \n",
      " |  \n",
      " |  INTERNAL_PROTOCOL = <class 'mrjob.protocol.UltraJSONProtocol'>\n",
      " |      Implements :py:class:`JSONProtocol` using the :py:mod:`ujson` library.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          :py:mod:`ujson` is about five times faster than the standard\n",
      " |          implementation, but is more willing to encode things that aren't\n",
      " |          strictly JSON-encodable, including sets, dictionaries with\n",
      " |          tuples as keys, UTF-8 encoded bytes, and objects (!). Relying on this\n",
      " |          behavior won't stop your job from working, but it can\n",
      " |          make your job *dependent* on :py:mod:`ujson`, rather than just using\n",
      " |          it as a speedup.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          :py:mod:`ujson` also differs from the standard implementation in that\n",
      " |          it doesn't  add spaces to its JSONs (``{\"foo\":\"bar\"}`` versus\n",
      " |          ``{\"foo\": \"bar\"}``). This probably won't affect anything but test\n",
      " |          cases and readability.\n",
      " |  \n",
      " |  \n",
      " |  JOBCONF = {}\n",
      " |  \n",
      " |  LIBJARS = []\n",
      " |  \n",
      " |  OUTPUT_PROTOCOL = <class 'mrjob.protocol.UltraJSONProtocol'>\n",
      " |      Implements :py:class:`JSONProtocol` using the :py:mod:`ujson` library.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          :py:mod:`ujson` is about five times faster than the standard\n",
      " |          implementation, but is more willing to encode things that aren't\n",
      " |          strictly JSON-encodable, including sets, dictionaries with\n",
      " |          tuples as keys, UTF-8 encoded bytes, and objects (!). Relying on this\n",
      " |          behavior won't stop your job from working, but it can\n",
      " |          make your job *dependent* on :py:mod:`ujson`, rather than just using\n",
      " |          it as a speedup.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          :py:mod:`ujson` also differs from the standard implementation in that\n",
      " |          it doesn't  add spaces to its JSONs (``{\"foo\":\"bar\"}`` versus\n",
      " |          ``{\"foo\": \"bar\"}``). This probably won't affect anything but test\n",
      " |          cases and readability.\n",
      " |  \n",
      " |  \n",
      " |  PARTITIONER = None\n",
      " |  \n",
      " |  SORT_VALUES = None\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(MRJob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/count_line.Karez.20240416.081227.486835\n",
      "Running step 1 of 1...\n",
      "job output is in /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/count_line.Karez.20240416.081227.486835/output\n",
      "Streaming final output from /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/count_line.Karez.20240416.081227.486835/output...\n",
      "\"sum of lines:\"\t4\n",
      "Removing temp directory /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/count_line.Karez.20240416.081227.486835...\n"
     ]
    }
   ],
   "source": [
    "!python count_line.py test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/count_line_and_chars.Karez.20240416.081227.949317\n",
      "Running step 1 of 1...\n",
      "job output is in /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/count_line_and_chars.Karez.20240416.081227.949317/output\n",
      "Streaming final output from /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/count_line_and_chars.Karez.20240416.081227.949317/output...\n",
      "\"lines\"\t6\n",
      "\"chars\"\t132\n",
      "Removing temp directory /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/count_line_and_chars.Karez.20240416.081227.949317...\n"
     ]
    }
   ],
   "source": [
    "!python count_line_and_chars.py count_line_and_chars.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/count_line_and_chars_words.Karez.20240416.081228.452920\n",
      "Running step 1 of 1...\n",
      "job output is in /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/count_line_and_chars_words.Karez.20240416.081228.452920/output\n",
      "Streaming final output from /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/count_line_and_chars_words.Karez.20240416.081228.452920/output...\n",
      "\"words\"\t18\n",
      "\"lines\"\t16\n",
      "\"chars\"\t243\n",
      "Removing temp directory /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/count_line_and_chars_words.Karez.20240416.081228.452920...\n"
     ]
    }
   ],
   "source": [
    "!python count_line_and_chars_words.py count_line_and_chars_words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spam.py\n"
     ]
    }
   ],
   "source": [
    "%%file spam.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRspam(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield \"line\", 1\n",
    "        yield \"words\", len(line.split())\n",
    "if __name__ == \"__main__\":\n",
    "    MRspam.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spam.py\n"
     ]
    }
   ],
   "source": [
    "%%file spam.py\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        words = re.findall(r'\\w+', line.lower())\n",
    "        for word in words:\n",
    "            if word == \"she\":\n",
    "                yield \"she\", 1\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRWordCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"spam jsnakldbsfnclksanbj fvlhekc\"\n",
    "\n",
    "word.startswith(\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spam.py\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spam.py\n"
     ]
    }
   ],
   "source": [
    "%%file spam.py\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        # Using a regular expression to find words in the line\n",
    "        words = re.findall(r'\\b\\w+\\b', line.lower())  # This correctly finds all words\n",
    "        for word in words:\n",
    "            if word == \"spam\" or word == \"ham\":\n",
    "                yield word, 1\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRWordCount.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spam.py\n"
     ]
    }
   ],
   "source": [
    "%%file spam.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRSpamPercentage(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count,\n",
    "                   reducer=self.reducer_count),\n",
    "            MRStep(reducer=self.reducer_percentage)\n",
    "        ]\n",
    "\n",
    "    def mapper_count(self, _, line):\n",
    "        label, _ = line.split('\\t')\n",
    "        yield \"total\", 1\n",
    "        if label == \"spam\":\n",
    "            yield \"spam\", 1\n",
    "\n",
    "    def reducer_count(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "    def reducer_percentage(self, key, values):\n",
    "        # Initialize counters to zero\n",
    "        total_count = 0\n",
    "        spam_count = 0\n",
    "\n",
    "        # Iterate through values to find total and spam counts\n",
    "        for count in values:\n",
    "            if key == \"total\":\n",
    "                total_count += count\n",
    "            elif key == \"spam\":\n",
    "                spam_count += count\n",
    "\n",
    "        # Only output percentage if there was at least one \"total\" count\n",
    "        if total_count > 0:\n",
    "            spam_percentage = (spam_count / total_count) * 100\n",
    "            yield \"Spam Percentage\", f\"{spam_percentage:.2f}%\"\n",
    "        else:\n",
    "            yield \"Spam Percentage\", \"No messages found\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRSpamPercentage.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spam.py\n"
     ]
    }
   ],
   "source": [
    "%%file spam.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRSpamPercentage(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count,\n",
    "                   reducer=self.reducer_count),\n",
    "            MRStep(reducer=self.reducer_percentage)\n",
    "        ]\n",
    "\n",
    "    def mapper_count(self, _, line):\n",
    "        label = line.split('\\t')[0]\n",
    "        yield \"total\", 1\n",
    "        if label == \"spam\":\n",
    "            yield \"spam\", 1\n",
    "\n",
    "    def reducer_count(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "    def reducer_percentage(self, _, counts):\n",
    "        total_count = 0\n",
    "        spam_count = 0\n",
    "        for key, count in counts:\n",
    "            if key == \"total\":\n",
    "                total_count = count\n",
    "            elif key == \"spam\":\n",
    "                spam_count = count\n",
    "\n",
    "        if total_count > 0:\n",
    "            spam_percentage = (spam_count / total_count) * 100\n",
    "            yield \"Spam Percentage\", f\"{spam_percentage:.2f}%\"\n",
    "        else:\n",
    "            yield \"Spam Percentage\", \"No messages found\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRSpamPercentage.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer_percentage(self, key, values):\n",
    "    # Initialize counters to zero\n",
    "    total_count = 0\n",
    "    spam_count = 0\n",
    "\n",
    "    # Iterate through values to find total and spam counts\n",
    "    for count in values:\n",
    "        if key == \"total\":\n",
    "            total_count += count\n",
    "        elif key == \"spam\":\n",
    "            spam_count += count\n",
    "\n",
    "    # Only output percentage if there was at least one \"total\" count\n",
    "    if total_count > 0:\n",
    "        spam_percentage = (spam_count / total_count) * 100\n",
    "        yield \"Spam Percentage\", f\"{spam_percentage:.2f}%\"\n",
    "    else:\n",
    "        yield \"Spam Percentage\", \"No messages found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spam.py\n"
     ]
    }
   ],
   "source": [
    "%%file spam.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRSpamPercentage(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_filter_lines,\n",
    "                   reducer=self.reducer_count_labels),\n",
    "            MRStep(reducer=self.reducer_calculate_percentage)\n",
    "        ]\n",
    "\n",
    "    def mapper_filter_lines(self, _, line):\n",
    "        line = line.lower()\n",
    "        if line.startswith('spam') or line.startswith(''):\n",
    "            label = line.split()[0] \n",
    "            yield label, 1\n",
    "\n",
    "    def reducer_count_labels(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "    def reducer_calculate_percentage(self, key, values):\n",
    "        counts = list(values)\n",
    "        if key == \"spam\":\n",
    "            self.spam_count = sum(counts)\n",
    "        elif key == \"ham\":\n",
    "            self.ham_count = sum(counts)\n",
    "\n",
    "        if hasattr(self, 'spam_count') and hasattr(self, 'ham_count'):\n",
    "            total = self.spam_count + self.ham_count\n",
    "            if total > 0:\n",
    "                spam_percentage = (self.spam_count / total) * 100\n",
    "                yield \"Spam Percentage\", f\"{spam_percentage:.2f}%\"\n",
    "            else:\n",
    "                yield \"Spam Percentage\", \"No messages to process\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRSpamPercentage.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spam.py\n"
     ]
    }
   ],
   "source": [
    "%%file spam.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRSpamPercentage(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_filter_lines,\n",
    "                   reducer=self.reducer_count_labels),\n",
    "            MRStep(reducer=self.reducer_calculate_percentage)\n",
    "        ]\n",
    "\n",
    "    def mapper_filter_lines(self, _, line):\n",
    "        line = line.lower()\n",
    "        if line.startswith('spam') or line.startswith('ham'):\n",
    "            label = line.split()[0]  # Extracts 'spam' or 'ham'\n",
    "            yield label, 1\n",
    "\n",
    "    def reducer_count_labels(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "    def reducer_calculate_percentage(self, key, values):\n",
    "        # Store counts in an instance variable accessible across the reducer calls\n",
    "        setattr(self, f\"{key}_count\", sum(values))\n",
    "        # Ensure both spam and ham counts are set\n",
    "        try:\n",
    "            if hasattr(self, 'spam_count') and hasattr(self, 'ham_count'):\n",
    "                total = self.spam_count + self.ham_count\n",
    "                if total > 0:\n",
    "                    spam_percentage = (self.spam_count / total) * 100\n",
    "                    yield \"Spam Percentage\", f\"{spam_percentage:.2f}%\"\n",
    "        except Exception as e:\n",
    "            # Catch any error that might occur and yield it for debugging purposes\n",
    "            yield \"Error\", str(e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRSpamPercentage.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spam.py\n"
     ]
    }
   ],
   "source": [
    "%%file spam.py\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        line = line.lower()\n",
    "        if line.startswith('spam'):\n",
    "            yield \"spam\", 1\n",
    "        elif line.startswith('ham'):\n",
    "            yield \"ham\", 1\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "        yield word, \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRWordCount.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spam.py\n"
     ]
    }
   ],
   "source": [
    "%%file spam.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class MRSpamFilter(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper_get_words,\n",
    "                reducer=self.reducer_count_words\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.mapper_get_keys,\n",
    "                reducer=self.reducer_output\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # Rozdzielanie linii na sowa z pominiciem znakw niealfanumerycznych\n",
    "        words = re.findall(r'[a-z]+', line.lower())\n",
    "        for word in words:\n",
    "            yield word, 1\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # Zliczanie wystpie sw\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def mapper_get_keys(self, word, count):\n",
    "        # Mapowanie sw na klucze \"common\" (10 najczstszych) i \"unique\" (wystpujce tylko raz)\n",
    "        if count == 1:\n",
    "            yield \"unique\", word\n",
    "        else:\n",
    "            yield \"common\", (count, word)\n",
    "\n",
    "    def reducer_output(self, key, values):\n",
    "        if key == \"unique\":\n",
    "            # Zliczanie sw wystpujcych tylko raz\n",
    "            unique_words = list(values)\n",
    "            yield \"Count of unique words\", len(unique_words)\n",
    "        else:\n",
    "            # Wywietlanie 10 najczstszych sw\n",
    "            top10 = Counter(dict(values)).most_common(10)\n",
    "            yield \"Top 10 common words\", [word for count, word in top10]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRSpamFilter.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/spam.Karez.20240423.074944.165968\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/spam.Karez.20240423.074944.165968/output\n",
      "Streaming final output from /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/spam.Karez.20240423.074944.165968/output...\n",
      "\"Count of unique words\"\t3855\n",
      "\"Top 10 common words\"\t[\"your\",\"you\",\"yes\",\"www\",\"work\",\"won\",\"with\",\"why\",\"who\",\"where\"]\n",
      "Removing temp directory /var/folders/b5/vt5tbdrd3vsb9sjq0cdwyc900000gn/T/spam.Karez.20240423.074944.165968...\n"
     ]
    }
   ],
   "source": [
    "!python spam.py SMSSpam.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
